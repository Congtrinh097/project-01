# CV Analyzer - Full Stack Application

A full-stack web application that allows users to upload their CVs and receive AI-powered analysis using OpenAI's GPT-4o-mini model. The application extracts strengths and areas for improvement from uploaded CVs.

## Demo

Web App: https://cv-analyzer-frontend-68349950319.us-central1.run.app/

## Features

- **CV Upload**: Upload PDF or DOCX files up to 10MB
- **AI Analysis**: Uses OpenAI GPT-4o-mini to analyze CV content
- **Semantic Search**: Find similar CVs using vector embeddings and pgvector
- **CV Recommendation**: Get AI-powered candidate recommendations
- **Resume Generation**: Generate professional resumes with AI assistance
- **AI Chatbot**: Interactive chatbot for career advice and CV questions
- **Text Extraction**: Automatically extracts text from PDF and DOCX files
- **Database Storage**: Stores CV metadata and analysis results in PostgreSQL
- **Modern UI**: Clean, responsive interface built with React and TailwindCSS
- **Real-time Progress**: Shows upload and analysis progress
- **History Management**: View previously uploaded CVs and their analyses

## Tech Stack

### Backend

- **FastAPI**: Modern Python web framework
- **SQLAlchemy**: ORM for database operations
- **PostgreSQL**: Primary database
- **OpenAI API**: AI-powered CV analysis
- **pdfplumber**: PDF text extraction
- **python-docx**: DOCX text extraction

### Frontend

- **React 18**: Modern React with hooks
- **Vite**: Fast build tool and dev server
- **TailwindCSS**: Utility-first CSS framework
- **React Query**: Data fetching and caching
- **Axios**: HTTP client for API calls
- **Lucide React**: Modern icon library

### Infrastructure

- **Docker & Docker Compose**: Containerization and orchestration
- **PostgreSQL**: Database service with pgvector extension
- **Google Cloud Run**: Production deployment (optional)

## Quick Start

### Prerequisites

- Docker and Docker Compose installed
- OpenAI API key

### Setup

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd cv-analyzer
   ```

2. **Configure environment variables**

   ```bash
   cp env.example .env
   ```

   Edit `.env` and add your OpenAI API key:

   ```
   OPENAI_API_KEY=your_openai_api_key_here
   ```

3. **Start the application**

   ```bash
   docker-compose up --build
   ```

4. **Access the application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

## Deployment

### ðŸš€ Deploy to Google Cloud Run

For production deployment, we provide automated deployment to Google Cloud Run:

**Quick Deploy** (15 minutes):

```bash
# Windows
deploy-cloudrun.bat

# Linux/Mac
chmod +x deploy-cloudrun.sh
./deploy-cloudrun.sh
```

**Documentation**:

- ðŸ“– [Complete Deployment Guide](docs/GOOGLE_CLOUD_RUN_DEPLOYMENT.md) - Detailed step-by-step instructions
- âš¡ [Quick Start Guide](docs/QUICK_START_CLOUDRUN.md) - Get running in 15 minutes

**Estimated Costs**:

- ðŸ†“ **FREE Option**: $0-5/month with Supabase ([Cost Guide](docs/DEPLOYMENT_COST_SUMMARY.md))
- ðŸ’° **Cloud SQL**: $15-50/month for low to moderate usage

**Features**:

- âœ… Automatic scaling (scale to zero)
- âœ… HTTPS enabled by default
- âœ… Cloud SQL with pgvector
- âœ… Secret Manager for API keys
- âœ… CI/CD ready with Cloud Build
- âœ… Global CDN support

### Other Deployment Options

- **AWS**: Deploy to ECS/Fargate with RDS PostgreSQL
- **Azure**: Use Azure Container Instances with Azure Database for PostgreSQL
- **DigitalOcean**: App Platform or Droplets with Managed PostgreSQL
- **Heroku**: Simple deployment with Heroku Postgres

## API Endpoints

### Upload CV

```http
POST /upload-cv
Content-Type: multipart/form-data

Body: file (PDF or DOCX)
```

### Get CV Analysis

```http
GET /cv/{id}
```

### List All CVs

```http
GET /cv
```

### Health Check

```http
GET /health
```

## Project Structure

```
cv-analyzer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â”œâ”€â”€ database.py             # Database connection
â”‚   â”œâ”€â”€ models.py               # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas.py              # Pydantic schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ cv_analyzer.py      # OpenAI integration
â”‚   â”‚   â””â”€â”€ file_processor.py   # File processing
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ Dockerfile              # Backend container
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main React component
â”‚   â”‚   â”œâ”€â”€ main.jsx            # React entry point
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js          # API client
â”‚   â”‚   â””â”€â”€ index.css           # Global styles
â”‚   â”œâ”€â”€ package.json            # Node dependencies
â”‚   â””â”€â”€ Dockerfile              # Frontend container
â”œâ”€â”€ docker-compose.yml          # Service orchestration
â”œâ”€â”€ env.example                 # Environment template
â””â”€â”€ README.md                   # This file
```

## Database Schema

### CVs Table

- `id`: Primary key (integer)
- `filename`: Original filename (string)
- `file_path`: Local file path (string)
- `file_size`: File size in bytes (integer)
- `upload_time`: Upload timestamp (datetime)
- `summary_pros`: AI analysis - strengths (text)
- `summary_cons`: AI analysis - areas for improvement (text)

## Configuration

### Environment Variables

| Variable             | Description                  | Default                                                     |
| -------------------- | ---------------------------- | ----------------------------------------------------------- |
| `OPENAI_API_KEY`     | OpenAI API key (required)    | -                                                           |
| `DATABASE_URL`       | PostgreSQL connection string | `postgresql://postgres:password@localhost:5432/cv_analyzer` |
| `UPLOAD_DIR`         | Directory for file storage   | `/tmp/uploads`                                              |
| `MAX_FILE_SIZE`      | Maximum file size in bytes   | `10485760` (10MB)                                           |
| `ALLOWED_EXTENSIONS` | Allowed file extensions      | `pdf,docx`                                                  |

### File Upload Limits

- Maximum file size: 10MB
- Supported formats: PDF, DOCX
- Files are stored temporarily in `/tmp/uploads/`

## Development

### Backend Development

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend Development

```bash
cd frontend
npm install
npm run dev
```

### Database Migration

The application automatically creates database tables on startup. For production, consider using Alembic for proper migrations.

## Production Considerations

1. **Security**: Add authentication and authorization
2. **File Storage**: Use cloud storage (S3, GCS) instead of local files
3. **Database**: Use managed PostgreSQL service
4. **Monitoring**: Add logging and monitoring
5. **Rate Limiting**: Implement API rate limiting
6. **SSL**: Use HTTPS in production
7. **Environment**: Use proper environment management

## Troubleshooting

### Common Issues

1. **OpenAI API Key Error**

   - Ensure your API key is valid and has sufficient credits
   - Check the `.env` file has the correct key

2. **File Upload Fails**

   - Check file size (max 10MB)
   - Ensure file is PDF or DOCX format
   - Verify upload directory permissions

3. **Database Connection Error**

   - Ensure PostgreSQL is running
   - Check database credentials in `.env`

4. **Frontend Can't Connect to Backend**
   - Verify both services are running
   - Check CORS configuration
   - Ensure correct API URL in frontend

## License

This project is licensed under the MIT License.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request
